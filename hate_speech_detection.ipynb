{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d716a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25cddb",
   "metadata": {},
   "source": [
    "### Importing tenosrflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81ae89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15f5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cd2ae",
   "metadata": {},
   "source": [
    "### Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1607b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d0b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf18fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']\n",
    "Y = [1 if i == \"hate\" else 0 for i in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca776a65",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded192b",
   "metadata": {},
   "source": [
    "### Converting to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bea25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lower text\n",
    "def toLower(data):\n",
    "    res = []\n",
    "    for sentence in data:\n",
    "        res.append(str.lower(sentence))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609fb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanset = toLower(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522c9b5",
   "metadata": {},
   "source": [
    "### Importing nltk libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd043947",
   "metadata": {},
   "source": [
    "### Removing stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b718ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "filtered_sentences = []\n",
    "\n",
    "# Removing stopwords\n",
    "for text in cleanset:\n",
    "    content = []\n",
    "    for i in word_tokenize(text):\n",
    "        if i not in stop_words:\n",
    "            content.append(i)\n",
    "    filtered_sentences.append(' '.join(content))\n",
    "    \n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "filtered_sentences_2 = []\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "for sentence in filtered_sentences:\n",
    "    new_words = tokenizer.tokenize(sentence)\n",
    "    filtered_sentences_2.append(' '.join(new_words))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3242d05",
   "metadata": {},
   "source": [
    "### Removing numbers and text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f248b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove numbers\n",
    "#nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "def remove_numbers(text):\n",
    "    # define the pattern to keep\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', text)\n",
    " \n",
    "#nltk.download('wordnet')\n",
    "filtered_sentences_2 = [remove_numbers(sentence) for sentence in filtered_sentences_2]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "filtered_sentences_2 = [lemmatizer.lemmatize(sentence) for sentence in filtered_sentences_2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f02039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife',\n",
       " 'dalits lowlife scum',\n",
       " 'dalits lowlives',\n",
       " 'better world women dare question men']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_2[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8307c07",
   "metadata": {},
   "source": [
    "### Filtering useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ad40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences_3 = []\n",
    "\n",
    "for sentence in filtered_sentences_2:\n",
    "    wordsToAdd = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        if len(word) >= 3 and word.isalpha():\n",
    "            wordsToAdd.append(word)\n",
    "    filtered_sentences_3.append(' '.join(wordsToAdd))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06f64acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife', 'dalits lowlife scum', 'dalits lowlives']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_3[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3ab37",
   "metadata": {},
   "source": [
    "### Removing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4bc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences_3 = [sentence.strip() for sentence in filtered_sentences_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196fafe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife',\n",
       " 'dalits lowlife scum',\n",
       " 'dalits lowlives',\n",
       " 'better world women dare question']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c6c5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = filtered_sentences_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c6c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067ae1b",
   "metadata": {},
   "source": [
    "### Function for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c941097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTagging(sentences):\n",
    "    final = []\n",
    "    for sentence in sentences:\n",
    "        res = []\n",
    "        tok=nltk.tokenize.word_tokenize(sentence) \n",
    "        pos=nltk.pos_tag(tok)\n",
    "        for token in pos:\n",
    "            res.append(token[0] + \"_\" + token[1])\n",
    "        final.append(' '.join(res))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80eaadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tagged_corpus = posTagging(corpus)\n",
    "#print(tagged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7bf65",
   "metadata": {},
   "source": [
    "### Creating BoW model with term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "702a195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=30000, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=30000,ngram_range=(1,3))\n",
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78284e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = len(vect.vocabulary_)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca0f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f747b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4649c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548adae",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb644e8c",
   "metadata": {},
   "source": [
    "### One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edaa018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = size\n",
    "\n",
    "onehot_repr = [one_hot(sentence, vocab_size) for words in corpus]\n",
    "\n",
    "#onehot_repr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc34e21",
   "metadata": {},
   "source": [
    "### Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d641ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 15\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre',maxlen=sent_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ae94eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 60)            1800000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               208800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,009,001\n",
      "Trainable params: 2,009,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Model Creation\n",
    "embedding_vector_features = 60\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features, input_length=sent_length))\n",
    "model.add(LSTM(200)) ## 200 neurons\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a70d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_for_LSTM = np.array(embedded_docs)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1696ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_for_LSTM,Y,test_size=0.2, random_state=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a099641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "508/508 [==============================] - 16s 29ms/step - loss: 0.6891 - accuracy: 0.5463 - val_loss: 0.6910 - val_accuracy: 0.5330\n",
      "Epoch 2/10\n",
      "508/508 [==============================] - 14s 27ms/step - loss: 0.6889 - accuracy: 0.5475 - val_loss: 0.6910 - val_accuracy: 0.5330\n",
      "Epoch 3/10\n",
      "508/508 [==============================] - 13s 26ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6911 - val_accuracy: 0.5330\n",
      "Epoch 4/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6911 - val_accuracy: 0.5330\n",
      "Epoch 5/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6916 - val_accuracy: 0.5330\n",
      "Epoch 6/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6921 - val_accuracy: 0.5330\n",
      "Epoch 7/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6922 - val_accuracy: 0.5330\n",
      "Epoch 8/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6913 - val_accuracy: 0.5330\n",
      "Epoch 9/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6917 - val_accuracy: 0.5330\n",
      "Epoch 10/10\n",
      "508/508 [==============================] - 13s 26ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6916 - val_accuracy: 0.5330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2049d91adc0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training deep learning model\n",
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b4690",
   "metadata": {},
   "source": [
    "### Performance Metrics and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dd08fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46695384615384616\n"
     ]
    }
   ],
   "source": [
    "predict_x=model.predict(X_test) \n",
    "Y_pred=np.argmax(predict_x,axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75deed33",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c37766e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5384615384615384"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=25)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "#print(Y_train[:4])\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y_test, clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad72caa",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4396ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355076923076923"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logReg = LogisticRegression(max_iter = 1000)\n",
    "logReg = logReg.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "accuracy_score(Y_test, logReg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed7e2e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00778960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decTree = DecisionTreeClassifier()\n",
    "\n",
    "decTree = decTree.fit(X_train, Y_train)\n",
    "\n",
    "accuracy_score(Y_test, decTree.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f7378",
   "metadata": {},
   "source": [
    "## Basic Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78b30992",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=25)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7708ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "#import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adc3bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2233, 1561],\n",
       "       [1427, 2904]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "metrics.confusion_matrix(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76171060",
   "metadata": {},
   "source": [
    "## Passive Aggressive Classifier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9086c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "clf = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74fe80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.708\n",
      "[[2559 1235]\n",
      " [1141 3190]]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "pred = linear_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07290e3b",
   "metadata": {},
   "source": [
    "## Multinomial Classifier with Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb6c2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b427d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0, Score : 0.6162461538461539\n",
      "Alpha: 0.1, Score : 0.6201846153846153\n",
      "Alpha: 0.2, Score : 0.6209230769230769\n",
      "Alpha: 0.30000000000000004, Score : 0.6216615384615385\n",
      "Alpha: 0.4, Score : 0.6247384615384616\n",
      "Alpha: 0.5, Score : 0.6264615384615385\n",
      "Alpha: 0.6000000000000001, Score : 0.6274461538461539\n",
      "Alpha: 0.7000000000000001, Score : 0.6296615384615385\n",
      "Alpha: 0.8, Score : 0.6291692307692308\n",
      "Alpha: 0.9, Score : 0.6312615384615384\n"
     ]
    }
   ],
   "source": [
    "previous_score=0\n",
    "for alpha in np.arange(0,1,0.1):\n",
    "    sub_classifier=MultinomialNB(alpha=alpha)\n",
    "    sub_classifier.fit(X_train,y_train)\n",
    "    y_pred=sub_classifier.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    if score>previous_score:\n",
    "        classifier=sub_classifier\n",
    "    print(\"Alpha: {}, Score : {}\".format(alpha,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f3e5b",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567cbee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7edc6f5cb888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train,y_train)\n",
    "print(accuracy_score(y_test,clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
