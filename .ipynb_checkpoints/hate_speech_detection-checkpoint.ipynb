{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d25cddb",
   "metadata": {},
   "source": [
    "### Importing tenosrflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81ae89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de514c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15f5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cd2ae",
   "metadata": {},
   "source": [
    "### Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1607b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d0b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf18fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']\n",
    "Y = [1 if i == \"hate\" else 0 for i in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca776a65",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded192b",
   "metadata": {},
   "source": [
    "### Converting to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bea25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lower text\n",
    "def toLower(data):\n",
    "    res = []\n",
    "    for sentence in data:\n",
    "        res.append(str.lower(sentence))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609fb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanset = toLower(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522c9b5",
   "metadata": {},
   "source": [
    "### Importing nltk libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd043947",
   "metadata": {},
   "source": [
    "### Removing stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b718ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "filtered_sentences = []\n",
    "\n",
    "# Removing stopwords\n",
    "for text in cleanset:\n",
    "    content = []\n",
    "    for i in word_tokenize(text):\n",
    "        if i not in stop_words:\n",
    "            content.append(i)\n",
    "    filtered_sentences.append(' '.join(content))\n",
    "    \n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "filtered_sentences_2 = []\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "for sentence in filtered_sentences:\n",
    "    new_words = tokenizer.tokenize(sentence)\n",
    "    filtered_sentences_2.append(' '.join(new_words))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3242d05",
   "metadata": {},
   "source": [
    "### Removing numbers and text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f248b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove numbers\n",
    "import re\n",
    "\n",
    "def remove_numbers(text):\n",
    "    # define the pattern to keep\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', text)\n",
    " \n",
    "#nltk.download('wordnet')\n",
    "filtered_sentences_2 = [remove_numbers(sentence) for sentence in filtered_sentences_2]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "filtered_sentences_2 = [lemmatizer.lemmatize(sentence) for sentence in filtered_sentences_2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f02039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife',\n",
       " 'dalits lowlife scum',\n",
       " 'dalits lowlives',\n",
       " 'better world women dare question men']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_2[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8307c07",
   "metadata": {},
   "source": [
    "### Filtering useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ad40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences_3 = []\n",
    "\n",
    "for sentence in filtered_sentences_2:\n",
    "    wordsToAdd = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        if len(word) >= 3 and word.isalpha():\n",
    "            wordsToAdd.append(word)\n",
    "    filtered_sentences_3.append(' '.join(wordsToAdd))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06f64acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife', 'dalits lowlife scum', 'dalits lowlives']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_3[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3ab37",
   "metadata": {},
   "source": [
    "### Removing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4bc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences_3 = [sentence.strip() for sentence in filtered_sentences_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196fafe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife',\n",
       " 'dalits lowlife scum',\n",
       " 'dalits lowlives',\n",
       " 'better world women dare question']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c6c5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = filtered_sentences_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4c6c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067ae1b",
   "metadata": {},
   "source": [
    "### Function for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c941097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTagging(sentences):\n",
    "    final = []\n",
    "    for sentence in sentences:\n",
    "        res = []\n",
    "        tok=nltk.tokenize.word_tokenize(sentence) \n",
    "        pos=nltk.pos_tag(tok)\n",
    "        for token in pos:\n",
    "            res.append(token[0] + \"_\" + token[1])\n",
    "        final.append(' '.join(res))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80eaadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_corpus = posTagging(corpus)\n",
    "print(tagged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7bf65",
   "metadata": {},
   "source": [
    "### Creating BoW model with term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "702a195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=20000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=20000,ngram_range=(1,1))\n",
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78284e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19646"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = len(vect.vocabulary_)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ca0f128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbos',\n",
       " 'abhorrent',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'able get',\n",
       " 'able take',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abomination',\n",
       " 'aboriginal',\n",
       " 'aboriginal people',\n",
       " 'aboriginals',\n",
       " 'abortion',\n",
       " 'abroad',\n",
       " 'absolute',\n",
       " 'absolute shit',\n",
       " 'absolutely',\n",
       " 'absolutely adore',\n",
       " 'absolutely adore plan',\n",
       " 'absolutely fantastic',\n",
       " 'absolutely fantastic love',\n",
       " 'absolutely fucking',\n",
       " 'absolutely hilarious',\n",
       " 'absolutely love',\n",
       " 'absolutely love plan',\n",
       " 'absolutely nothing',\n",
       " 'absolutely stunning',\n",
       " 'absolutely stunning turned',\n",
       " 'absurd',\n",
       " 'abundance',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusers',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academics',\n",
       " 'academics therefore',\n",
       " 'academics therefore experts',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'accept kinds',\n",
       " 'accept kinds immigrants',\n",
       " 'accept others',\n",
       " 'accept others call',\n",
       " 'accept people',\n",
       " 'accept people calling',\n",
       " 'accept races',\n",
       " 'accept races equal',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accomplished',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accounts',\n",
       " 'accurate',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'ace',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'acknowledge',\n",
       " 'acquire',\n",
       " 'across',\n",
       " 'across whole',\n",
       " 'across world',\n",
       " 'act',\n",
       " 'act like',\n",
       " 'act normal',\n",
       " 'act stop',\n",
       " 'act stop need',\n",
       " 'act way',\n",
       " 'act way communities',\n",
       " 'acting',\n",
       " 'acting like',\n",
       " 'acting like one',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actions debate',\n",
       " 'actions debate always',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'actively encourage',\n",
       " 'actively encourage extinguishing',\n",
       " 'activism',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actual fuck',\n",
       " 'actually',\n",
       " 'actually eliminate',\n",
       " 'actually like',\n",
       " 'actually need',\n",
       " 'actually people',\n",
       " 'actually respect',\n",
       " 'actually think',\n",
       " 'actually want',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adhere',\n",
       " 'adhere british',\n",
       " 'adhere british values',\n",
       " 'administration',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adolf',\n",
       " 'adopt',\n",
       " 'adopting',\n",
       " 'adore',\n",
       " 'adore idea',\n",
       " 'adore idea burning',\n",
       " 'adore plan',\n",
       " 'adore plan kill',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advantage',\n",
       " 'advantage others',\n",
       " 'advantage worth',\n",
       " 'advantage worth genuine',\n",
       " 'advert',\n",
       " 'advert diverse',\n",
       " 'advert diverse everyone',\n",
       " 'advertising',\n",
       " 'adverts',\n",
       " 'adverts celebrated',\n",
       " 'adverts celebrated yawn',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advocacy',\n",
       " 'advocacy project',\n",
       " 'advocate',\n",
       " 'advocating',\n",
       " 'advocating free',\n",
       " 'advocating free school',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affection',\n",
       " 'affects',\n",
       " 'afford',\n",
       " 'afghani',\n",
       " 'afghani people',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'afraid black',\n",
       " 'afraid black people',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'african americans',\n",
       " 'african people',\n",
       " 'african women',\n",
       " 'african women educationalists',\n",
       " 'africans',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'ages',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agree disagree',\n",
       " 'agree disagree wrong',\n",
       " 'agree immigration',\n",
       " 'agree rapists',\n",
       " 'agree rapists scum',\n",
       " 'agree whatever',\n",
       " 'agree whatever comes',\n",
       " 'agreed',\n",
       " 'agriculture',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'aka',\n",
       " 'alarm',\n",
       " 'alcohol',\n",
       " 'ali',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allah worshippers',\n",
       " 'alleys',\n",
       " 'alleys friendly',\n",
       " 'alleys friendly people',\n",
       " 'allow',\n",
       " 'allow people',\n",
       " 'allow people like',\n",
       " 'allowed',\n",
       " 'allowed adopt',\n",
       " 'allowed breathe',\n",
       " 'allowed breathe anymore',\n",
       " 'allowed call',\n",
       " 'allowed country',\n",
       " 'allowed exist',\n",
       " 'allowed exist feel',\n",
       " 'allowed exist that',\n",
       " 'allowed express',\n",
       " 'allowed express opinion',\n",
       " 'allowed kids',\n",
       " 'allowed kids parties',\n",
       " 'allowed live',\n",
       " 'allowed live anymore',\n",
       " 'allowed park',\n",
       " 'allowed say',\n",
       " 'allowed social',\n",
       " 'allowed social network',\n",
       " 'allowed vote',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allows free',\n",
       " 'allows free speech',\n",
       " 'almost',\n",
       " 'almost like',\n",
       " 'alone',\n",
       " 'alone allowed',\n",
       " 'alone allowed exist',\n",
       " 'along',\n",
       " 'along lines',\n",
       " 'alongside',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'already',\n",
       " 'already aware',\n",
       " 'already aware obvious',\n",
       " 'already know',\n",
       " 'already merely',\n",
       " 'already merely expressed',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'also know',\n",
       " 'also like',\n",
       " 'also love',\n",
       " 'also said',\n",
       " 'also verge',\n",
       " 'also verge extinction',\n",
       " 'although',\n",
       " 'although people',\n",
       " 'although people belligerent',\n",
       " 'although wrong',\n",
       " 'altogether',\n",
       " 'always',\n",
       " 'always end',\n",
       " 'always end covering',\n",
       " 'always found',\n",
       " 'always get',\n",
       " 'always lazy',\n",
       " 'always makes',\n",
       " 'always say',\n",
       " 'always say right',\n",
       " 'always trying',\n",
       " 'always trying get',\n",
       " 'always used',\n",
       " 'always yet',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazing best',\n",
       " 'amazing best kindest',\n",
       " 'amazing landscapes',\n",
       " 'amazing landscapes friendly',\n",
       " 'amazing nothing',\n",
       " 'amazing nothing wrong',\n",
       " 'amazingly',\n",
       " 'amazingly incredibly',\n",
       " 'amazingly incredibly angry',\n",
       " 'amazon',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amish',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amongst various',\n",
       " 'amongst various populations',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amusing',\n",
       " 'anal',\n",
       " 'anatomy',\n",
       " 'ancestors',\n",
       " 'ancestral',\n",
       " 'ancestral roots',\n",
       " 'ancestry',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anger',\n",
       " 'anglo',\n",
       " 'anglo saxon',\n",
       " 'angry',\n",
       " 'angry black',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'announced',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoying explain',\n",
       " 'annoying explain reasons',\n",
       " 'annoys',\n",
       " 'another',\n",
       " 'another come',\n",
       " 'another come along',\n",
       " 'another man',\n",
       " 'another one',\n",
       " 'another reason',\n",
       " 'another reason inferiority',\n",
       " 'answer',\n",
       " 'anti',\n",
       " 'anti black',\n",
       " 'anti fascist',\n",
       " 'anti sematic',\n",
       " 'anti semitic',\n",
       " 'anti semitism',\n",
       " 'anti trans',\n",
       " 'anti white',\n",
       " 'antifa',\n",
       " 'antisemitism',\n",
       " 'antithesis',\n",
       " 'antithesis emotional',\n",
       " 'antithesis emotional stability',\n",
       " 'antithesis hard',\n",
       " 'antithesis hard work',\n",
       " 'antithesis intelligence',\n",
       " 'antithesis purity',\n",
       " 'antithetical',\n",
       " 'antithetical concepts',\n",
       " 'antithetical concepts beauty',\n",
       " 'antithetical concepts integrity',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyone calling',\n",
       " 'anyone calling asian',\n",
       " 'anyone else',\n",
       " 'anyone else getting',\n",
       " 'anyone else love',\n",
       " 'anyone get',\n",
       " 'anyone really',\n",
       " 'anyone says',\n",
       " 'anyone says black',\n",
       " 'anyone says native',\n",
       " 'anyone think',\n",
       " 'anyone thinks',\n",
       " 'anyone would',\n",
       " 'anyone would silly',\n",
       " 'anything',\n",
       " 'anything else',\n",
       " 'anything like',\n",
       " 'anything like unlikely',\n",
       " 'anything want',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'anywhere else',\n",
       " 'anywhere else world',\n",
       " 'anywhere near',\n",
       " 'apart',\n",
       " 'apartheid',\n",
       " 'apartment',\n",
       " 'ape',\n",
       " 'apes',\n",
       " 'app',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'apparently pls',\n",
       " 'apparently pls educate',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'applaud',\n",
       " 'applaud murder',\n",
       " 'apple',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approximately',\n",
       " 'apps',\n",
       " 'arab',\n",
       " 'arab men',\n",
       " 'arab people',\n",
       " 'arabia',\n",
       " 'arabic',\n",
       " 'arabic people',\n",
       " 'arabs',\n",
       " 'arabs like',\n",
       " 'arbitrary',\n",
       " 'area',\n",
       " 'area compared',\n",
       " 'area compared used',\n",
       " 'area compared years',\n",
       " 'area gone',\n",
       " 'area gone downhill',\n",
       " 'area lot',\n",
       " 'areas',\n",
       " 'arent',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'arguing random',\n",
       " 'arguing random people',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'arguments today',\n",
       " 'arguments today already',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'aroma',\n",
       " 'around',\n",
       " 'around black',\n",
       " 'around informing',\n",
       " 'around informing people',\n",
       " 'around world',\n",
       " 'around world find',\n",
       " 'around world picked',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arse',\n",
       " 'arsehole',\n",
       " 'arseholes',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'article',\n",
       " 'arts',\n",
       " 'arts philosophy',\n",
       " 'arts philosophy war',\n",
       " 'aryan',\n",
       " 'asap',\n",
       " 'asda',\n",
       " 'asexual',\n",
       " 'ashamed',\n",
       " 'asia',\n",
       " 'asia disproportionally',\n",
       " 'asian',\n",
       " 'asian families',\n",
       " 'asian girls',\n",
       " 'asian men',\n",
       " 'asian people',\n",
       " 'asian woman',\n",
       " 'asian women',\n",
       " 'asians',\n",
       " 'asians like',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'ass',\n",
       " 'assault',\n",
       " 'asses',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'assigned',\n",
       " 'assimilate',\n",
       " 'assistance',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assure',\n",
       " 'astonishing',\n",
       " 'asylum',\n",
       " 'asylum seeker',\n",
       " 'asylum seekers',\n",
       " 'asylum seekers simply',\n",
       " 'ate',\n",
       " 'atheist',\n",
       " 'atheists',\n",
       " 'athletes',\n",
       " 'atmosphere',\n",
       " 'atrocities',\n",
       " 'attack',\n",
       " 'attack black',\n",
       " 'attack everyone',\n",
       " 'attack everyone adhere',\n",
       " 'attacked',\n",
       " 'attacking',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attention',\n",
       " 'attention soon',\n",
       " 'attitude',\n",
       " 'attitude makes',\n",
       " 'attitude makes think',\n",
       " 'attitude towards',\n",
       " 'attitudes',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'audacity',\n",
       " 'audience',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'australians',\n",
       " 'authentic',\n",
       " 'authority',\n",
       " 'autism',\n",
       " 'autistic',\n",
       " 'automatically',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'awake',\n",
       " 'aware',\n",
       " 'aware feel',\n",
       " 'aware obvious',\n",
       " 'aware obvious explanations',\n",
       " 'awareness',\n",
       " 'awareness enables',\n",
       " 'awareness enables resist',\n",
       " 'away',\n",
       " 'away england',\n",
       " 'away england nowadays',\n",
       " 'away people',\n",
       " 'away really',\n",
       " 'away really ends',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awful although',\n",
       " 'awful although wrong',\n",
       " 'awful feel',\n",
       " 'awful feel safe',\n",
       " 'awful wan',\n",
       " 'awful wan die',\n",
       " 'aws',\n",
       " 'babies',\n",
       " 'baboons',\n",
       " 'baby',\n",
       " 'baby boomers',\n",
       " 'back',\n",
       " 'back africa',\n",
       " 'back belong',\n",
       " 'back belong ugly',\n",
       " 'back came',\n",
       " 'back china',\n",
       " 'back countries',\n",
       " 'back country',\n",
       " 'back home',\n",
       " 'back take',\n",
       " 'back take beauties',\n",
       " 'back tears',\n",
       " 'backbone',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'bad bitch',\n",
       " 'bad people',\n",
       " 'bad thing',\n",
       " 'bad things',\n",
       " 'badass',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'bait',\n",
       " 'ball',\n",
       " 'ballots',\n",
       " 'balls',\n",
       " 'bame',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'bananas',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bangladesh',\n",
       " 'bangladeshi',\n",
       " 'bangladeshis',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'bankrupt',\n",
       " 'banks',\n",
       " 'banned',\n",
       " 'banned country',\n",
       " 'banned life',\n",
       " 'banning',\n",
       " 'bar',\n",
       " 'bar kicks',\n",
       " 'bar kicks wtf',\n",
       " 'barbarians',\n",
       " 'barbaric',\n",
       " 'barbie',\n",
       " 'barbie dolls',\n",
       " 'barbies',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'barren',\n",
       " 'barren woman',\n",
       " 'barren women',\n",
       " 'bars',\n",
       " 'bartender',\n",
       " 'bartender says',\n",
       " 'bartender says want',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basic human',\n",
       " 'basically',\n",
       " 'basically mites',\n",
       " 'basically mites lice',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'basketball team',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " 'bat',\n",
       " 'bat eaters',\n",
       " 'bathroom',\n",
       " 'bats',\n",
       " 'batshit',\n",
       " 'batty',\n",
       " 'batty boy',\n",
       " 'batty boys',\n",
       " 'battyboys',\n",
       " 'bbc',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'bear mind',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beasts',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beaten enough',\n",
       " 'beating',\n",
       " 'beauties',\n",
       " 'beauties world',\n",
       " 'beauties world issue',\n",
       " 'beautiful',\n",
       " 'beautiful brother',\n",
       " 'beautiful brother converted',\n",
       " 'beautiful country',\n",
       " 'beautiful little',\n",
       " 'beautiful little towns',\n",
       " 'beautiful particular',\n",
       " 'beautiful particular today',\n",
       " 'beautiful sister',\n",
       " 'beautiful sister converted',\n",
       " 'beautiful woman',\n",
       " 'beautiful woman talking',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'became extinct',\n",
       " 'become',\n",
       " 'become much',\n",
       " 'become much diverse',\n",
       " 'become one',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedbugs',\n",
       " 'beds',\n",
       " 'beer',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'behave',\n",
       " 'behave like',\n",
       " 'behaving',\n",
       " 'behaving like',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'behaviours',\n",
       " 'beheaded',\n",
       " 'beheading',\n",
       " 'behind',\n",
       " 'beings',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'believe black',\n",
       " 'believe blacks',\n",
       " 'believe fucking',\n",
       " 'believe jews',\n",
       " 'believe jews exist',\n",
       " 'believe people',\n",
       " 'believe world',\n",
       " 'believe world due',\n",
       " 'believed',\n",
       " 'believers',\n",
       " 'believes',\n",
       " 'believing',\n",
       " 'bellend',\n",
       " 'bellends',\n",
       " 'belligerent',\n",
       " 'belligerent could',\n",
       " 'belligerent could conducted',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belong fucking',\n",
       " 'belong jungle',\n",
       " 'belong ugly',\n",
       " 'belong wild',\n",
       " 'belong zoo',\n",
       " 'belong zoo yelled',\n",
       " 'belongs',\n",
       " 'bender',\n",
       " 'benders',\n",
       " 'beneath',\n",
       " 'beneath didnt',\n",
       " 'beneath didnt understand',\n",
       " 'beneficial',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'bent',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'best friend',\n",
       " 'best friends',\n",
       " 'best friends love',\n",
       " 'best kindest',\n",
       " 'best kindest smartest',\n",
       " 'best music',\n",
       " 'best music festivals',\n",
       " 'best thing',\n",
       " 'best things',\n",
       " 'best things earth',\n",
       " 'best universities',\n",
       " 'best universities world',\n",
       " 'best way',\n",
       " 'best world',\n",
       " 'best world offer',\n",
       " 'bet',\n",
       " 'bet next',\n",
       " 'bet next time',\n",
       " 'beta',\n",
       " 'betrayal',\n",
       " 'better',\n",
       " 'better black',\n",
       " 'better dogs',\n",
       " 'better get',\n",
       " 'better men',\n",
       " 'better place',\n",
       " 'better place without',\n",
       " 'better watch',\n",
       " 'better without',\n",
       " 'better women',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bias',\n",
       " 'bible',\n",
       " 'biden',\n",
       " 'big',\n",
       " 'big black',\n",
       " 'big deal',\n",
       " 'big mistake',\n",
       " 'big problem',\n",
       " 'big time',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggest issue',\n",
       " 'biggest issue living',\n",
       " 'bigot',\n",
       " 'bigoted',\n",
       " 'bigotry',\n",
       " 'bigots',\n",
       " 'bike',\n",
       " 'bikini',\n",
       " 'bill',\n",
       " 'bill gates',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'bin',\n",
       " 'binary',\n",
       " 'binary people',\n",
       " 'biological',\n",
       " 'biologically',\n",
       " 'biology',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bisexual',\n",
       " 'bisexual people',\n",
       " 'bisexuals',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitch like',\n",
       " 'bitches',\n",
       " 'bitching',\n",
       " 'bitchute',\n",
       " 'bitchy',\n",
       " 'bite',\n",
       " 'bitter',\n",
       " 'black',\n",
       " 'black americans',\n",
       " 'black barbie',\n",
       " 'black barbies',\n",
       " 'black birds',\n",
       " 'black black',\n",
       " 'black blokes',\n",
       " 'black brown',\n",
       " 'black cars',\n",
       " 'black cat',\n",
       " 'black cats',\n",
       " 'black chicks',\n",
       " 'black children',\n",
       " 'black community',\n",
       " 'black dog',\n",
       " 'black dress',\n",
       " 'black dude',\n",
       " 'black dudes',\n",
       " 'black filthy',\n",
       " 'black filthy colour',\n",
       " 'black folk',\n",
       " 'black friday',\n",
       " 'black friend',\n",
       " 'black friends',\n",
       " 'black fucking',\n",
       " 'black gets',\n",
       " 'black gets dirty',\n",
       " 'black girl',\n",
       " 'black girls',\n",
       " 'black guy',\n",
       " 'black guys',\n",
       " 'black hole',\n",
       " 'black kids',\n",
       " 'black lives',\n",
       " 'black lives matter',\n",
       " 'black makes',\n",
       " 'black makes look',\n",
       " 'black man',\n",
       " 'black man black',\n",
       " 'black men',\n",
       " 'black men actually',\n",
       " 'black men black',\n",
       " 'black men look',\n",
       " 'black minority',\n",
       " 'black minority ethnic',\n",
       " 'black ones',\n",
       " 'black panther',\n",
       " 'black panthers',\n",
       " 'black people',\n",
       " 'black people always',\n",
       " 'black people area',\n",
       " 'black people compared',\n",
       " 'black people country',\n",
       " 'black people deserve',\n",
       " 'black people good',\n",
       " 'black people inferior',\n",
       " 'black people like',\n",
       " 'black people look',\n",
       " 'black people much',\n",
       " 'black people really',\n",
       " 'black people would',\n",
       " 'black person',\n",
       " 'black population',\n",
       " 'black racis',\n",
       " 'black rat',\n",
       " 'black rats',\n",
       " 'black sexier',\n",
       " 'black sexier white',\n",
       " 'black skin',\n",
       " 'black trash',\n",
       " 'black white',\n",
       " 'black widow',\n",
       " 'black widows',\n",
       " 'black woman',\n",
       " 'black woman black',\n",
       " 'black women',\n",
       " 'black women actually',\n",
       " 'black women associated',\n",
       " 'black women black',\n",
       " 'black women look',\n",
       " 'black women much',\n",
       " 'black would',\n",
       " 'blackies',\n",
       " 'blacks',\n",
       " 'blacks die',\n",
       " 'blacks dumb',\n",
       " 'blacks dumb inferior',\n",
       " 'blacks great',\n",
       " 'blacks inferior',\n",
       " 'blacks know',\n",
       " 'blacks like',\n",
       " 'blacks live',\n",
       " 'blacks need',\n",
       " 'blacks whites',\n",
       " 'blacks would',\n",
       " 'blah',\n",
       " 'blah blah',\n",
       " 'blame',\n",
       " 'blaming',\n",
       " 'blanket',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'blend',\n",
       " 'blender',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessed mendacity',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'blm',\n",
       " 'blm stands',\n",
       " 'bloated',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blockers',\n",
       " 'bloke',\n",
       " 'blokes',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'bloodshed',\n",
       " 'bloodshed romanians',\n",
       " 'bloody',\n",
       " 'bloody hell',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'blue eyed',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'boat nigger',\n",
       " 'boat niggers',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'bojo',\n",
       " 'bollocks',\n",
       " 'bomb',\n",
       " 'bombing',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bone body',\n",
       " 'bones',\n",
       " 'boobs',\n",
       " 'booing',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boomer',\n",
       " 'boomers',\n",
       " 'boost',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'bored arguing',\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f747b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4649c602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(x)):\\n    if x[i] >= 1:\\n       print(\"Found one\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "777eb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Baby', 'IN'), ('we', 'PRP'), ('done', 'VBN'), ('it', 'PRP')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = word_tokenize(\"Baby we done it\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "pos_tag(tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548adae",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb644e8c",
   "metadata": {},
   "source": [
    "### One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edaa018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = size\n",
    "\n",
    "onehot_repr = [one_hot(sentence, vocab_size) for words in corpus]\n",
    "\n",
    "#onehot_repr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc34e21",
   "metadata": {},
   "source": [
    "### Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d641ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 15\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre',maxlen=sent_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae94eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 60)            1178760   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               208800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,387,761\n",
      "Trainable params: 1,387,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Model Creation\n",
    "embedding_vector_features = 60\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features, input_length=sent_length))\n",
    "model.add(LSTM(200)) ## 200 neurons\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a70d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_for_LSTM = np.array(embedded_docs)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1696ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_for_LSTM,Y,test_size=0.2, random_state=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a099641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "508/508 [==============================] - 14s 25ms/step - loss: 0.6891 - accuracy: 0.5472 - val_loss: 0.6935 - val_accuracy: 0.5330\n",
      "Epoch 2/10\n",
      "508/508 [==============================] - 12s 24ms/step - loss: 0.6889 - accuracy: 0.5475 - val_loss: 0.6910 - val_accuracy: 0.5330\n",
      "Epoch 3/10\n",
      "508/508 [==============================] - 12s 24ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6921 - val_accuracy: 0.5330\n",
      "Epoch 4/10\n",
      "508/508 [==============================] - 12s 24ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6913 - val_accuracy: 0.5330\n",
      "Epoch 5/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6911 - val_accuracy: 0.5330\n",
      "Epoch 6/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6913 - val_accuracy: 0.5330\n",
      "Epoch 7/10\n",
      "508/508 [==============================] - 13s 25ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6913 - val_accuracy: 0.5330\n",
      "Epoch 8/10\n",
      "508/508 [==============================] - 12s 24ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6914 - val_accuracy: 0.5330\n",
      "Epoch 9/10\n",
      "508/508 [==============================] - 12s 24ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6921 - val_accuracy: 0.5330\n",
      "Epoch 10/10\n",
      "508/508 [==============================] - 12s 24ms/step - loss: 0.6887 - accuracy: 0.5475 - val_loss: 0.6914 - val_accuracy: 0.5330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ca0667eb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training deep learning model\n",
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b4690",
   "metadata": {},
   "source": [
    "### Performance Metrics and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dd08fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46695384615384616\n"
     ]
    }
   ],
   "source": [
    "predict_x=model.predict(X_test) \n",
    "Y_pred=np.argmax(predict_x,axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75deed33",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c37766e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5344"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=25)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "#print(Y_train[:4])\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y_test, clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad72caa",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4396ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166769230769231"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logReg = LogisticRegression(max_iter = 1000)\n",
    "logReg = logReg.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "accuracy_score(Y_test, logReg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed7e2e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00778960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decTree = DecisionTreeClassifier()\n",
    "\n",
    "decTree = decTree.fit(X_train, Y_train)\n",
    "\n",
    "accuracy_score(Y_test, decTree.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7388465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
