{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d716a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25cddb",
   "metadata": {},
   "source": [
    "### Importing tenosrflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81ae89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15f5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cd2ae",
   "metadata": {},
   "source": [
    "### Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1607b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d0b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = df['text']\n",
    "testing_indices = []\n",
    "training_indices = []\n",
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,9] == 'test':\n",
    "        testing_indices.append(i)\n",
    "    else:\n",
    "        training_indices.append(i)\n",
    "        \n",
    "testing_data = df[df['split'] == 'test']\n",
    "training_data = df[(df['split'].isin(['train','dev']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf18fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']\n",
    "Y = [1 if i == \"hate\" else 0 for i in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca776a65",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded192b",
   "metadata": {},
   "source": [
    "### Converting to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bea25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lower text\n",
    "def toLower(data):\n",
    "    res = []\n",
    "    for sentence in data:\n",
    "        res.append(str.lower(sentence))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609fb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanset = toLower(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522c9b5",
   "metadata": {},
   "source": [
    "### Importing nltk libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd043947",
   "metadata": {},
   "source": [
    "### Removing stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b718ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "filtered_sentences = []\n",
    "\n",
    "# Removing stopwords\n",
    "for text in cleanset:\n",
    "    content = []\n",
    "    for i in word_tokenize(text):\n",
    "        if i not in stop_words:\n",
    "            content.append(i)\n",
    "    filtered_sentences.append(' '.join(content))\n",
    "    \n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "filtered_sentences_2 = []\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "for sentence in filtered_sentences:\n",
    "    new_words = tokenizer.tokenize(sentence)\n",
    "    filtered_sentences_2.append(' '.join(new_words))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3242d05",
   "metadata": {},
   "source": [
    "### Removing numbers and text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f248b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove numbers\n",
    "#nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "def remove_numbers(text):\n",
    "    # define the pattern to keep\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', text)\n",
    " \n",
    "#nltk.download('wordnet')\n",
    "filtered_sentences_2 = [remove_numbers(sentence) for sentence in filtered_sentences_2]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "filtered_sentences_2 = [lemmatizer.lemmatize(sentence) for sentence in filtered_sentences_2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12f02039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_sentences_2[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8307c07",
   "metadata": {},
   "source": [
    "### Filtering useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ad40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences_3 = []\n",
    "\n",
    "for sentence in filtered_sentences_2:\n",
    "    wordsToAdd = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        if len(word) >= 3 and word.isalpha():\n",
    "            wordsToAdd.append(word)\n",
    "    filtered_sentences_3.append(' '.join(wordsToAdd))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06f64acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife', 'dalits lowlife scum', 'dalits lowlives']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_3[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3ab37",
   "metadata": {},
   "source": [
    "### Removing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4bc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences_3 = [sentence.strip() for sentence in filtered_sentences_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196fafe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legal husband rape wife',\n",
       " 'dalits lowlife scum',\n",
       " 'dalits lowlives',\n",
       " 'better world women dare question']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_sentences_3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6c5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = filtered_sentences_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c6c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067ae1b",
   "metadata": {},
   "source": [
    "### Function for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c941097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTagging(sentences):\n",
    "    final = []\n",
    "    for sentence in sentences:\n",
    "        res = []\n",
    "        tok=nltk.tokenize.word_tokenize(sentence) \n",
    "        pos=nltk.pos_tag(tok)\n",
    "        for token in pos:\n",
    "            res.append(token[0] + \"_\" + token[1])\n",
    "        final.append(' '.join(res))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80eaadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tagged_corpus = posTagging(corpus)\n",
    "#print(tagged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7bf65",
   "metadata": {},
   "source": [
    "### Creating BoW model with term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "702a195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=30000, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=30000,ngram_range=(1,3))\n",
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78284e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = len(vect.vocabulary_)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ca0f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f747b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4649c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548adae",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb644e8c",
   "metadata": {},
   "source": [
    "### One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edaa018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = size\n",
    "\n",
    "onehot_repr = [one_hot(sentence, vocab_size) for words in corpus]\n",
    "\n",
    "#onehot_repr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc34e21",
   "metadata": {},
   "source": [
    "### Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d641ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 15\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre',maxlen=sent_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae94eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 60)            1800000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               208800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,009,001\n",
      "Trainable params: 2,009,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Model Creation\n",
    "embedding_vector_features = 60\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features, input_length=sent_length))\n",
    "model.add(LSTM(200)) ## 200 neurons\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a70d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_for_LSTM = np.array(embedded_docs)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1696ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([X_for_LSTM[i] for i in range(len(X_for_LSTM)) if i in training_indices])\n",
    "X_test = np.array([X_for_LSTM[i] for i in range(len(X_for_LSTM)) if i in testing_indices])\n",
    "#X_test = [i for i in X_for_LSTM if i in testing_indices]\n",
    "Y_train = np.array([Y[i] for i in range(len(Y)) if i in training_indices])\n",
    "Y_test = np.array([Y[i] for i in range(len(Y)) if i in testing_indices])\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_for_LSTM,Y,test_size=0.2, random_state=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a099641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "572/572 [==============================] - 21s 35ms/step - loss: 0.6895 - accuracy: 0.5445 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 2/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6893 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 3/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6893 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 4/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6894 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 5/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6892 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 6/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6892 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 7/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6891 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 8/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6891 - accuracy: 0.5448 - val_loss: 0.6896 - val_accuracy: 0.5428\n",
      "Epoch 9/10\n",
      "572/572 [==============================] - 20s 35ms/step - loss: 0.6893 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n",
      "Epoch 10/10\n",
      "572/572 [==============================] - 20s 36ms/step - loss: 0.6892 - accuracy: 0.5448 - val_loss: 0.6895 - val_accuracy: 0.5428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d9c023550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training deep learning model\n",
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b4690",
   "metadata": {},
   "source": [
    "### Performance Metrics and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dd08fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63      1857\n",
      "           1       0.00      0.00      0.00      2205\n",
      "\n",
      "    accuracy                           0.46      4062\n",
      "   macro avg       0.23      0.50      0.31      4062\n",
      "weighted avg       0.21      0.46      0.29      4062\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predict_x=model.predict(X_test) \n",
    "Y_pred=np.argmax(predict_x,axis=1)\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75deed33",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c37766e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.60      1857\n",
      "           1       0.62      0.33      0.43      2205\n",
      "\n",
      "    accuracy                           0.53      4062\n",
      "   macro avg       0.56      0.55      0.51      4062\n",
      "weighted avg       0.56      0.53      0.51      4062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=25)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "X_train = [X[i] for i in range(len(X)) if i in training_indices]\n",
    "X_test = [X[i] for i in range(len(X)) if i in testing_indices]\n",
    "Y_train = [Y[i] for i in range(len(Y)) if i in training_indices]\n",
    "Y_test = [Y[i] for i in range(len(Y)) if i in testing_indices]\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "#print(Y_train[:4])\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(Y_test, clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad72caa",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4396ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      1857\n",
      "           1       0.90      0.91      0.90      2205\n",
      "\n",
      "    accuracy                           0.89      4062\n",
      "   macro avg       0.89      0.89      0.89      4062\n",
      "weighted avg       0.89      0.89      0.89      4062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=25)\n",
    "\n",
    "logReg = LogisticRegression(max_iter = 1000)\n",
    "logReg = logReg.fit(X, Y)\n",
    "\n",
    "pickle.dump((vect,logReg),open('log_reg_model.pkl','wb'))\n",
    "#log_from_pickle = pickle.loads(saved_model)\n",
    "\n",
    "print(classification_report(Y_test, logReg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed7e2e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00778960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decTree = DecisionTreeClassifier()\n",
    "\n",
    "decTree = decTree.fit(X_train, Y_train)\n",
    "\n",
    "accuracy_score(Y_test, decTree.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f7378",
   "metadata": {},
   "source": [
    "## Basic Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78b30992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=25)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7708ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "#import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adc3bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56      1857\n",
      "           1       0.63      0.63      0.63      2205\n",
      "\n",
      "    accuracy                           0.60      4062\n",
      "   macro avg       0.59      0.59      0.59      4062\n",
      "weighted avg       0.60      0.60      0.60      4062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "print(classification_report(Y_test,classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76171060",
   "metadata": {},
   "source": [
    "## Passive Aggressive Classifier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9086c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "clf = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74fe80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.55      0.62      1857\n",
      "           1       0.68      0.80      0.74      2205\n",
      "\n",
      "    accuracy                           0.69      4062\n",
      "   macro avg       0.69      0.68      0.68      4062\n",
      "weighted avg       0.69      0.69      0.68      4062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07290e3b",
   "metadata": {},
   "source": [
    "## Multinomial Classifier with Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb6c2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b427d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0, Score : 0.587149187592319\n",
      "Alpha: 0.1, Score : 0.5869030034465781\n",
      "Alpha: 0.2, Score : 0.5876415558838011\n",
      "Alpha: 0.30000000000000004, Score : 0.587149187592319\n",
      "Alpha: 0.4, Score : 0.5898572131954702\n",
      "Alpha: 0.5, Score : 0.5898572131954702\n",
      "Alpha: 0.6000000000000001, Score : 0.5925652387986213\n",
      "Alpha: 0.7000000000000001, Score : 0.5937961595273265\n",
      "Alpha: 0.8, Score : 0.5945347119645494\n",
      "Alpha: 0.9, Score : 0.5960118168389956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56      1857\n",
      "           1       0.63      0.63      0.63      2205\n",
      "\n",
      "    accuracy                           0.60      4062\n",
      "   macro avg       0.59      0.59      0.59      4062\n",
      "weighted avg       0.60      0.60      0.60      4062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_score=0\n",
    "for alpha in np.arange(0,1,0.1):\n",
    "    sub_classifier=MultinomialNB(alpha=alpha)\n",
    "    sub_classifier.fit(X_train,Y_train)\n",
    "    y_pred=sub_classifier.predict(X_test)\n",
    "    score = accuracy_score(Y_test, y_pred)\n",
    "    if score>previous_score:\n",
    "        classifier=sub_classifier\n",
    "    print(\"Alpha: {}, Score : {}\".format(alpha,score))\n",
    "    \n",
    "print(classification_report(Y_test,classifier.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
